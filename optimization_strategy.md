# Optimization Strategy for Large-Scale PDF Processing

## Executive Summary
After comprehensive analysis of your code, logs, and performance patterns, here's a prioritized optimization strategy to handle 1000s-10000s of files efficiently. Current average processing time is **5 seconds per file** with concurrency, limited by API response times (2-30+ seconds) and memory management issues.

## ðŸŽ¯ High Impact, Low Effort Changes (Implement First)

### 1. **Optimize PDF Page Extraction (Immediate 20-30% improvement)**
```python
# Current issue: Loading entire PDF bytes even for classification
# Solution: Only load required pages into memory

def extract_first_n_pages_pdf_optimized(pdf_path: str, max_pages: int) -> bytes:
    """Optimized: Don't load entire PDF into memory first"""
    source_doc = fitz.open(pdf_path)
    if len(source_doc) <= max_pages:
        # Direct conversion without intermediate loading
        return source_doc.tobytes()
    
    # Use PyMuPDF's select() for true zero-copy selection
    source_doc.select(range(max_pages))
    pdf_bytes = source_doc.tobytes()
    source_doc.close()
    return pdf_bytes
```

### 2. **Increase Concurrency Limits (Immediate 50-100% throughput increase)**
```python
# Current: MAX_CONCURRENT_CLASSIFY = 10, MAX_CONCURRENT_EXTRACT = 5
# Recommended based on AFC limit analysis:
MAX_CONCURRENT_CLASSIFY = 20  # Flash model can handle more
MAX_CONCURRENT_EXTRACT = 8    # Still conservative for Pro model

# Add adaptive concurrency based on error rates
class AdaptiveSemaphore:
    def __init__(self, initial_limit=10, max_limit=25):
        self.limit = initial_limit
        self.max_limit = max_limit
        self.semaphore = asyncio.Semaphore(initial_limit)
        self.error_count = 0
        self.success_count = 0
    
    async def acquire(self):
        await self.semaphore.acquire()
        
    def release(self, success=True):
        if success:
            self.success_count += 1
            if self.success_count > 10 and self.limit < self.max_limit:
                self.limit += 1
                self.semaphore = asyncio.Semaphore(self.limit)
        else:
            self.error_count += 1
            if self.error_count > 3 and self.limit > 5:
                self.limit -= 2
                self.semaphore = asyncio.Semaphore(self.limit)
```

### 3. **Stream Results to Disk (Prevent memory exhaustion)**
```python
# Current issue: Keeping all results in memory
# Solution: Write results immediately

class StreamingCSVWriter:
    def __init__(self, filepath, fieldnames):
        self.filepath = filepath
        self.fieldnames = fieldnames
        self.file = open(filepath, 'w', newline='', encoding='utf-8')
        self.writer = csv.DictWriter(self.file, fieldnames=fieldnames)
        self.writer.writeheader()
        self.lock = asyncio.Lock()
    
    async def write_row(self, row_dict):
        async with self.lock:
            self.writer.writerow(row_dict)
            self.file.flush()  # Ensure immediate write
    
    def close(self):
        self.file.close()

# Use in process_files_batch:
async def process_files_batch_streaming(files, client, semaphore, csv_writer, ...):
    # Process and write results immediately instead of accumulating
    for completed_task in asyncio.as_completed(tasks):
        metadata, result = await completed_task
        if result:
            await csv_writer.write_row(result)  # Stream to disk
            # Don't accumulate in memory
```

### 4. **Implement Chunked Processing (Essential for 10000+ files)**
```python
def chunk_files(files, chunk_size=500):
    """Process files in manageable chunks"""
    for i in range(0, len(files), chunk_size):
        yield files[i:i + chunk_size]

async def process_large_dataset(all_files, client, ...):
    for chunk_num, file_chunk in enumerate(chunk_files(all_files)):
        logging.info(f"Processing chunk {chunk_num + 1} ({len(file_chunk)} files)")
        
        # Process chunk
        results = await process_files_batch(file_chunk, ...)
        
        # Stream results to disk
        # Clear memory after each chunk
        del results
        
        # Optional: Add delay between chunks to respect rate limits
        if chunk_num < total_chunks - 1:
            await asyncio.sleep(2)
```

## ðŸš€ Medium Effort, High Impact Changes

### 5. **Optimize Resume Functionality with SQLite**
```python
import sqlite3
import aiosqlite

class ProcessingTracker:
    def __init__(self, db_path="processing_state.db"):
        self.db_path = db_path
        self._init_db()
    
    def _init_db(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute('''
            CREATE TABLE IF NOT EXISTS processing_state (
                file_path TEXT PRIMARY KEY,
                classification TEXT,
                extraction_status TEXT,
                processed_at TIMESTAMP,
                error_message TEXT
            )
        ''')
        conn.commit()
        conn.close()
    
    async def mark_classified(self, file_path, classification):
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute(
                "INSERT OR REPLACE INTO processing_state "
                "(file_path, classification, processed_at) VALUES (?, ?, ?)",
                (file_path, classification, datetime.now())
            )
            await db.commit()
    
    async def get_pending_extractions(self):
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute(
                "SELECT file_path, classification FROM processing_state "
                "WHERE classification IN ('vendor_invoice', 'employee_t&e') "
                "AND extraction_status IS NULL"
            )
            return await cursor.fetchall()
```

### 6. **Implement Connection Pooling with aiohttp**
```python
# Install: pip install google-genai[aiohttp]

from google.genai import types

# Configure for better performance
http_options = types.HttpOptions(
    async_client_args={
        'connector': aiohttp.TCPConnector(
            limit=100,  # Total connection pool limit
            limit_per_host=30,  # Per-host connection limit
            ttl_dns_cache=300,  # DNS cache timeout
            enable_cleanup_closed=True
        ),
        'timeout': aiohttp.ClientTimeout(total=60)
    }
)

client = genai.Client(
    api_key=API_KEY,
    http_options=http_options
)
```

### 7. **Add Request Batching for Small Files**
```python
async def batch_small_pdfs(pdf_paths, max_batch_size_mb=10):
    """Batch multiple small PDFs into single requests"""
    batches = []
    current_batch = []
    current_size = 0
    
    for pdf_path in pdf_paths:
        file_size = os.path.getsize(pdf_path) / (1024 * 1024)  # MB
        
        if current_size + file_size > max_batch_size_mb and current_batch:
            batches.append(current_batch)
            current_batch = [pdf_path]
            current_size = file_size
        else:
            current_batch.append(pdf_path)
            current_size += file_size
    
    if current_batch:
        batches.append(current_batch)
    
    return batches
```

## ðŸ”§ Advanced Optimizations (Higher Effort)

### 8. **Implement Worker Pool Pattern**
```python
async def worker_pool_processor(work_queue: asyncio.Queue, result_queue: asyncio.Queue, 
                               worker_id: int, client, semaphore):
    """Worker that processes items from queue"""
    while True:
        try:
            item = await work_queue.get()
            if item is None:  # Poison pill
                break
                
            # Process item
            result = await process_single_file(item, client, semaphore)
            await result_queue.put((item, result))
            
        except Exception as e:
            logging.error(f"Worker {worker_id} error: {e}")
        finally:
            work_queue.task_done()

async def process_with_workers(files, num_workers=10):
    work_queue = asyncio.Queue(maxsize=100)  # Limit queue size for memory
    result_queue = asyncio.Queue()
    
    # Start workers
    workers = [
        asyncio.create_task(
            worker_pool_processor(work_queue, result_queue, i, client, semaphore)
        )
        for i in range(num_workers)
    ]
    
    # Feed work to queue
    for file in files:
        await work_queue.put(file)
    
    # Send poison pills
    for _ in range(num_workers):
        await work_queue.put(None)
    
    # Wait for completion
    await asyncio.gather(*workers)
```

### 9. **Add Caching Layer for Repeated Processing**
```python
import hashlib
from functools import lru_cache

class ResultCache:
    def __init__(self, cache_dir="cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    def get_file_hash(self, file_path):
        """Generate hash of file content for cache key"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    async def get_cached_result(self, file_path, operation_type):
        file_hash = self.get_file_hash(file_path)
        cache_file = self.cache_dir / f"{file_hash}_{operation_type}.json"
        
        if cache_file.exists():
            with open(cache_file, 'r') as f:
                return json.load(f)
        return None
    
    async def cache_result(self, file_path, operation_type, result):
        file_hash = self.get_file_hash(file_path)
        cache_file = self.cache_dir / f"{file_hash}_{operation_type}.json"
        
        with open(cache_file, 'w') as f:
            json.dump(result, f)
```

### 10. **Implement Health Monitoring & Auto-Recovery**
```python
class HealthMonitor:
    def __init__(self, threshold_error_rate=0.2):
        self.total_requests = 0
        self.failed_requests = 0
        self.threshold = threshold_error_rate
        self.last_reset = time.time()
    
    def record_request(self, success: bool):
        self.total_requests += 1
        if not success:
            self.failed_requests += 1
        
        # Check health every 100 requests
        if self.total_requests % 100 == 0:
            error_rate = self.failed_requests / self.total_requests
            if error_rate > self.threshold:
                logging.warning(f"High error rate: {error_rate:.2%}")
                return "throttle"  # Signal to reduce concurrency
        
        # Reset stats every hour
        if time.time() - self.last_reset > 3600:
            self.total_requests = 0
            self.failed_requests = 0
            self.last_reset = time.time()
        
        return "healthy"
```

## ðŸ“Š Performance Impact Estimates

| Optimization | Implementation Effort | Performance Gain | Memory Reduction |
|--------------|----------------------|------------------|------------------|
| PDF Page Optimization | Low (1 hour) | 20-30% faster | 50% less memory |
| Increase Concurrency | Low (30 min) | 50-100% throughput | No change |
| Stream Results | Low (2 hours) | No change | 80% less memory |
| Chunked Processing | Low (2 hours) | Enables scale | 90% less memory |
| SQLite Resume | Medium (4 hours) | 10% faster resume | 95% less memory |
| Connection Pooling | Medium (2 hours) | 20-30% faster | No change |
| Worker Pool | High (6 hours) | 30-40% faster | Better control |
| Caching Layer | Medium (4 hours) | 50% on re-runs | No change |

## ðŸŽ¯ Implementation Priority

### Phase 1 (Immediate - 1 day):
1. Optimize PDF page extraction
2. Increase concurrency limits
3. Implement streaming CSV writer

### Phase 2 (Week 1):
4. Add chunked processing
5. Replace CSV resume with SQLite
6. Setup aiohttp connection pooling

### Phase 3 (Week 2):
7. Implement worker pool pattern
8. Add caching layer
9. Add health monitoring

## ðŸ’¡ Additional Recommendations

1. **Use environment variables for all limits**:
   ```python
   MAX_CONCURRENT_CLASSIFY = int(os.getenv("MAX_CONCURRENT_CLASSIFY", "20"))
   MAX_CONCURRENT_EXTRACT = int(os.getenv("MAX_CONCURRENT_EXTRACT", "8"))
   CHUNK_SIZE = int(os.getenv("PROCESSING_CHUNK_SIZE", "500"))
   ```

2. **Add progress persistence**:
   Save progress after each chunk to enable interruption/resume at any point.

3. **Consider multiprocessing for CPU-bound operations**:
   PDF parsing could benefit from process pool for true parallelism.

4. **Monitor API quotas**:
   Track daily/monthly quotas and throttle accordingly.

5. **Implement exponential backoff with jitter**:
   ```python
   delay = min(300, (2 ** attempt) + random.uniform(0, 3))
   ```

## ðŸš¨ Critical for 10000+ Files

- **MUST implement**: Streaming results, chunked processing, SQLite tracking
- **Memory budget**: Assume 4GB available, plan for 2GB working set
- **Checkpoint frequently**: Every 100-500 files
- **Use generators**: Never load all file paths into memory at once
- **Consider distributed processing**: For 100000+ files, consider Celery/Ray

## Questions to Consider

1. What's your actual AFC (API rate limit) quota? This affects optimal concurrency.
2. Are you running on a machine with SSD? This affects optimal chunk sizes.
3. Do you need real-time results or can processing run overnight?
4. Would you consider using multiple API keys for parallel processing?
5. Is the 5-second average acceptable, or do you need faster processing?

This strategy prioritizes changes that maintain your core pipeline while dramatically improving scalability and performance. Start with Phase 1 for immediate improvements, then progressively implement based on your scaling needs.